\documentclass[fleqn]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{changepage}
\title{Problem Set 2}
\author{Michael Tang}
\begin{document}
\maketitle
\pagenumbering{arabic}
\begin{adjustwidth}{-2cm}{-2cm}

\section{Problem 1: Stock \& Watson}
\subsection{Exercise 3.2}
\textbf{(a)}
\begin{align*}
\overline{Y} &= E\left[Y_{1},...,Y_{n}\right] = \hat{p}*1 + \left(1-\hat{p}\right)*0\\
&= \hat{p}
\end{align*}
\textbf{(b)}\\
$E\left[\overline{Y}\right] = E\left[\hat{p}\right] = \frac{1}{n}\sum_{i=1}^{n}E\left(Y_{i}\right) = \frac{1}{n}\sum_{i=1}^{n}p = p$\\
Therefore, $\hat{p}$ is an unbiased estimator of $p$.\\
\textbf{(c)}
\begin{align*}
var\left(\hat{p}\right) &= var\left(\overline{Y}\right) = var\left(\frac{1}{n}\sum_{i=1}^{n}Y_{i}\right) = \frac{1}{n^{2}}\sum_{i=1}^{n}var\left(Y_{i}\right)\\
&= \frac{1}{n^{2}}\sum_{i=1}^{n}\sigma_{Y}^2, \sigma_{Y} = p\left(1-p\right) \text{for Bernoulli rv.}\\
&= \frac{p\left(1-p\right)}{n}  \square
\end{align*}
\subsection{Exercise 3.3}
\textbf{(a)}\\
$\hat{p} = 215/400 = .538$\\
\textbf{(b)}\\
$SE\left(\hat{p}\right) = \sqrt{var\left(\hat{p}\right)} = \sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}} = \sqrt{\frac{.538*.462}{400}} = .025$\\
\textbf{(c)}\\
By the Central Limit Theorem, $\left\vert\frac{\hat{p}-\mu_{0}}{SE\left(\hat{p}\right)}\right\vert$ has approximate distribution $N\left(0,1\right)$.
\begin{align*}
p-value &= P_{H_{0}}\left\{\left\vert \frac{\hat{p}-\mu_{0}}{SE\left(\hat{p}\right)}\right\vert > \left\vert\frac{\hat{p}^{act}-\mu_{0}}{SE\left(\hat{p}^{act}\right)}\right\vert\right\}\\
&= 2\Phi\left(-\left\vert \frac{.538-.5}{.025} \right\vert\right) = 2\Phi\left(-1.52\right) = 2\left(.0643\right)\\
&= .1286
\end{align*}
\textbf{(d)}
\begin{align*}
p-value &= P_{H_{0}}\left\{\frac{\hat{p}-\mu_{0}}{SE\left(\hat{p}\right)} > \frac{\hat{p}^{act}-\mu_{0}}{SE\left(\hat{p}^{act}\right)}\right\}\\
&= \Phi\left(\frac{.038}{.025}\right) = \Phi\left(-1.52\right)\\
&= .0643
\end{align*}
\textbf{(e)}\\
p-value of (d) is one half that of (c) because it only counts one direction of inequality and not two.\\
\textbf{(f)}\\
Depends on the significance level one wants to take. Assuming a significance level $\alpha = .05$, both p-values show that we do not have significant enough evidence to suggest that the incumbent is ahead of the challenger.
\subsection{Exercise 3.4}
\textbf{(a)}\\
$95\%$ confidence for $p = \left\{\hat{p} \pm 1.96*SE\left(\hat{p}\right)\right\} = \left\{.538 \pm .049\right\} = \left\{.489, .587\right\}$\\
\textbf{(b)}\\
$99\%$ confidence for $p = \left\{.538 \pm 2.58*.025\right\} = \left\{.4735, .6025\right\}$\\
\textbf{(c)}\\
The interval in (b) is wider because to be able to capture the population fraction in a larger percentage of samples one has to expand the accepted range of values around the sample mean.\\
\textbf{(d)}\\
We cannot reject the null hypothesis at the $5\%$ significance level; see part (f) of Exercise 3.3.
\subsection{Exercise 3.15}
\textbf{(a)}
\begin{align*}
E\left(\hat{p}_{a}\right) &= \frac{1}{n_{a}}\sum_{i=1}^{n_{a}}E\left(Y_{i}\right) = \frac{1}{n_{a}}\sum_{i=1}^{n_{a}}p_{a} = p_{a}\\
var\left(\hat{p}_{a}\right) &= \frac{1}{n_{a}}\sum_{i=1}^{n_{a}}var\left(Y_{i}\right) = \frac{\sigma_{a}}{n_{a}} = \frac{p_{a}\left(1-p_{a}\right)}{n_{a}}\\
E\left(\hat{p}_{b}\right) &= \frac{1}{n_{b}}\sum_{i=1}^{n_{b}}p_{b} = p_{b}\\
var\left(\hat{p}_{b}\right) &= \frac{\sigma_{b}}{n_{a}} = \frac{p_{b}\left(1-p_{b}\right)}{n_{b}}
\end{align*}
\textbf{(b)}\\
Since samples are independent:
\begin{equation*}
var\left(\hat{p}_{a} - \hat{p}_{b}\right) = var\left(\hat{p}_{a}\right) - var\left(\hat{p}_{b}\right) = \frac{p_{a}\left(1-p_{a}\right)}{n_{a}} - \frac{p_{b}\left(1-p_{b}\right)}{n_{b}} \square
\end{equation*}
\textbf{(c)}
\begin{align*}
\text{95\% confidence for $p_{a}-p_{b}$} &= \left(\hat{p}_{a}-\hat{p}_{b}\right) \pm 1.96*SE\left(\hat{p}_{a}-\hat{p}_{b}\right)\\
&= \left(\hat{p}_{a}-\hat{p}_{b}\right) \pm 1.96\sqrt{SE\left(\hat{p}_{a}\right)^{2} - SE\left(\hat{p}_{b}\right)^{2}}\\
&= \left(\hat{p}_{a}-\hat{p}_{b}\right) \pm 1.96\sqrt{\frac{\hat{p}_{a}\left(1-\hat{p}_{a}\right)}{n_{a}} + \frac{\hat{p}_{b}\left(1-\hat{p}_{b}\right)}{n_{b}}}
\end{align*}
For 90\% confidence critical value = 2.58, so interval:\\
$\left(\hat{p}_{a}-\hat{p}_{b}\right) \pm 2.58\sqrt{\frac{\hat{p}_{a}\left(1-\hat{p}_{a}\right)}{n_{a}} + \frac{\hat{p}_{b}\left(1-\hat{p}_{b}\right)}{n_{b}}}$\\
\textbf{(d)}\\
$\left(.859-.374\right) \pm 1.96\sqrt{\frac{.859*.141}{5801} + \frac{.374*.626}{4249}} = [.468, .502]$
\subsection{Exercise 3.16}
\textbf{(a)}\\
$\mu_{Y} = 1000$ is the mean score for the population, and $\overline{Y} = 1013$ is the sample mean.
\begin{align*}
SE\left(\overline{Y}\right) &= \frac{s}{\sqrt{n}} = \frac{108}{\sqrt{453}} = 5.074\\
\text{95\% interval} &= \overline{Y} \pm 1.96*SE\left(\left(\overline{Y}\right)\right)\\
&= 1013 \pm 1.96*5.074 = [1003.054, 1022.946]
\end{align*}
\textbf{(b)}\\
There is statistically significant evidence of Florida students performing differently than other US students at 5\% significance, because the population mean of 1000 is outside the 95\% confidence interval.\\
\textbf{(c)}\\
\null\quad i. $(\overline{Y}_{p}-\overline{Y}) \pm 1.96*SE(\overline{Y}_{p}-\overline{Y}) = (1019-1013) \pm 1.96*\sqrt{95^{2}/503 + 108^{2}/453} = [-6.955, 18.955]$\\
\null\quad ii. Not at 5\% significance, because a difference of 0 between prepped and non-prepped students is within the 95\% confidence interval.\\
\textbf{(d)}\\
\null\quad i. $9 \pm 1.96*(60/\sqrt{453}) = [3.475, 14.525]$\\
\null\quad ii. Yes at 5\% significance, because difference of 0 is not within the 95\% confidence interval.\\
\null\quad iii. We already have the statistics for when prep is administered and students do not take the test twice. \null\quad\null\quad Now, to compare to that effect and also to the two effects combined, isolate the effect of taking twice \null\quad\null\quad by having another large sample group take the test twice with no prep.

\subsection{Exercise 3.17}
\textbf{(a)}\\
$(25.30-24.83) \pm 1.96\sqrt{12.09^{2}/2004 + 10.85^{2}/1594} = [-.281, 1.221]$\\
\textbf{(b)}\\
$(21.50-21.39) \pm 1.96\sqrt{9.99^{2}/1951 + 8.39^{2}/1368} = [-.518, .738]$\\
\textbf{(c)}\\
$(3.80-3.44) \pm 1.96\sqrt{.35^{2} + .35^{2}} = .36 \pm .97 = [-.61, 1.33]$

\subsection{Exercise 3.18}
\textbf{(a)}
\begin{align*}
E\left[\left(Y_{i} - \overline{Y}\right)^{2}\right] &= var\left(Y_{i} - \overline{Y}\right) - E\left[Y_{i} - \overline{Y}\right]^{2}\\
&= var\left(Y_{i}\right) - 2cov\left(Y_{i}, \overline{Y}\right) + var\left(\overline{Y}\right) - 0^{2} \\
&= var\left(Y_{i}\right) - 2cov\left(Y_{i}, \overline{Y}\right) + var\left(\overline{Y}\right) \square
\end{align*}
\textbf{(b)}
\begin{align*}
cov\left(\overline{Y}, Y_{i}\right) &= cov\left(\frac{1}{n}\sum_{j=1}^{n}, Y_{i}\right)\\
&= cov\left(\frac{Y_{i}}{n}, Y_{i}\right)\\
&= \frac{1}{n}cov\left(Y_{i}, Y_{i}\right) = \frac{\sigma_{Y}^{2}}{n}
\end{align*}
\textbf{(c)}
\begin{align*}
E\left(s_{y}^{2}\right) &= E\left(\frac{1}{n-1}\sum_{i=1}^{n}\left(Y_{i} - \overline{Y}\right)^{2}\right)\\
&= \frac{1}{n-1}\sum_{i=1}^{n}E\left[\left(Y_{i} - \overline{Y}\right)^{2}\right]\\
&= \frac{1}{n-1}\sum_{i=1}^{n}\left(var\left(Y_{i}\right) - 2cov\left(Y_{i}, \overline{Y}\right) + var\left(\overline{Y}\right)\right)\\
&= \frac{1}{n-1}\sum_{i=1}^{n}\left(\sigma_{Y}^{2} - 2*\frac{\sigma_{Y}^{2}}{n} + \frac{\sigma_{Y}^{2}}{n}\right)\\
&= \frac{1}{n-1}\sum_{i=1}^{n}\left(\frac{n-1}{n}\sigma_{Y}^{2}\right)\\
&= \frac{1}{n-1}*n*\left(\frac{n-1}{n}\sigma_{Y}^{2}\right) = \sigma_{Y}^{2}
\end{align*}

\section{Problem 2}
\textbf{(a)}\\
\textbf{(b)}\\
\textbf{(c)}\\
\textbf{(d)}\\
\textbf{(e)}\\
\textbf{(f)}\\

\section{Problem 3}
\textbf{(a)}\\
\textbf{(b)}\\
\textbf{(c)}\\

\section{Problem 4}
\textbf{(a)}\\
\textbf{(b)}\\
\textbf{(c)}\\

\section{Problem 5}
\textbf{(a)}\\
\textbf{(b)}\\

\end{adjustwidth}
\end{document}